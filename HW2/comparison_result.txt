epochs = 10
	learning rate = 0.01
		layers = [255]
			activation = sigmoid
				batch size = 128
					Adam=> acc: 0.975, avg recall: 0.975, avg precision: 0.975, avg f1: 0.975
					SGD => acc: 0.882, avg recall: 0.882, avg precision: 0.880, avg f1: 0.880
				batch size = 512
					Adam=> acc: 0.979, avg recall: 0.979, avg precision: 0.979, avg f1: 0.979
					SGD => acc: 0.776, avg recall: 0.795, avg precision: 0.770, avg f1: 0.764
				batch size = 2048
					Adam=> acc: 0.973, avg recall: 0.973, avg precision: 0.972, avg f1: 0.972
					SGD => acc: 0.542, avg recall: 0.562, avg precision: 0.531, avg f1: 0.507
			activation = relu
				batch size = 128
					Adam=> acc: 0.967, avg recall: 0.968, avg precision: 0.967, avg f1: 0.967
					SGD => acc: 0.923, avg recall: 0.922, avg precision: 0.922, avg f1: 0.922
				batch size = 512
					Adam=> acc: 0.976, avg recall: 0.976, avg precision: 0.976, avg f1: 0.976
					SGD => acc: 0.892, avg recall: 0.891, avg precision: 0.890, avg f1: 0.890
				batch size = 2048
					Adam=> acc: 0.978, avg recall: 0.978, avg precision: 0.977, avg f1: 0.978
					SGD => acc: 0.768, avg recall: 0.789, avg precision: 0.763, avg f1: 0.763
		layers = [64, 32]
			activation = sigmoid
				batch size = 128
					Adam=> acc: 0.970, avg recall: 0.970, avg precision: 0.969, avg f1: 0.969
					SGD => acc: 0.588, avg recall: 0.691, avg precision: 0.577, avg f1: 0.560
				batch size = 512
					Adam=> acc: 0.970, avg recall: 0.971, avg precision: 0.970, avg f1: 0.970
					SGD => acc: 0.146, avg recall: 0.302, avg precision: 0.132, avg f1: 0.061
				batch size = 2048
					Adam=> acc: 0.965, avg recall: 0.965, avg precision: 0.965, avg f1: 0.965
					SGD => acc: 0.103, avg recall: 0.100, avg precision: 0.100, avg f1: 0.019
			activation = relu
				batch size = 128
					Adam=> acc: 0.966, avg recall: 0.966, avg precision: 0.965, avg f1: 0.965
					SGD => acc: 0.925, avg recall: 0.925, avg precision: 0.925, avg f1: 0.925
				batch size = 512
					Adam=> acc: 0.967, avg recall: 0.968, avg precision: 0.967, avg f1: 0.967
					SGD => acc: 0.883, avg recall: 0.882, avg precision: 0.881, avg f1: 0.881
				batch size = 2048
					Adam=> acc: 0.971, avg recall: 0.970, avg precision: 0.971, avg f1: 0.970
					SGD => acc: 0.440, avg recall: 0.495, avg precision: 0.434, avg f1: 0.412
		layers = [32, 32, 32]
			activation = sigmoid
				batch size = 128
					Adam=> acc: 0.956, avg recall: 0.956, avg precision: 0.956, avg f1: 0.956
					SGD => acc: 0.113, avg recall: 0.101, avg precision: 0.100, avg f1: 0.020
				batch size = 512
					Adam=> acc: 0.958, avg recall: 0.958, avg precision: 0.957, avg f1: 0.957
					SGD => acc: 0.113, avg recall: 0.101, avg precision: 0.100, avg f1: 0.020
				batch size = 2048
					Adam=> acc: 0.957, avg recall: 0.956, avg precision: 0.957, avg f1: 0.956
					SGD => acc: 0.098, avg recall: 0.100, avg precision: 0.100, avg f1: 0.018
			activation = relu
				batch size = 128
					Adam=> acc: 0.965, avg recall: 0.964, avg precision: 0.965, avg f1: 0.964
					SGD => acc: 0.926, avg recall: 0.925, avg precision: 0.925, avg f1: 0.925
				batch size = 512
					Adam=> acc: 0.958, avg recall: 0.960, avg precision: 0.957, avg f1: 0.958
					SGD => acc: 0.846, avg recall: 0.847, avg precision: 0.843, avg f1: 0.842
				batch size = 2048
					Adam=> acc: 0.963, avg recall: 0.963, avg precision: 0.963, avg f1: 0.963
					SGD => acc: 0.199, avg recall: 0.122, avg precision: 0.204, avg f1: 0.121
	learning rate = 0.001
		layers = [255]
			activation = sigmoid
				batch size = 128
					Adam=> acc: 0.973, avg recall: 0.974, avg precision: 0.973, avg f1: 0.973
					SGD => acc: 0.656, avg recall: 0.675, avg precision: 0.645, avg f1: 0.625
				batch size = 512
					Adam=> acc: 0.957, avg recall: 0.957, avg precision: 0.957, avg f1: 0.957
					SGD => acc: 0.113, avg recall: 0.199, avg precision: 0.111, avg f1: 0.038
				batch size = 2048
					Adam=> acc: 0.930, avg recall: 0.930, avg precision: 0.929, avg f1: 0.929
					SGD => acc: 0.090, avg recall: 0.093, avg precision: 0.091, avg f1: 0.028
			activation = relu
				batch size = 128
					Adam=> acc: 0.980, avg recall: 0.981, avg precision: 0.980, avg f1: 0.980
					SGD => acc: 0.823, avg recall: 0.827, avg precision: 0.819, avg f1: 0.818
				batch size = 512
					Adam=> acc: 0.976, avg recall: 0.975, avg precision: 0.976, avg f1: 0.975
					SGD => acc: 0.551, avg recall: 0.617, avg precision: 0.548, avg f1: 0.538
				batch size = 2048
					Adam=> acc: 0.960, avg recall: 0.960, avg precision: 0.960, avg f1: 0.960
					SGD => acc: 0.156, avg recall: 0.232, avg precision: 0.155, avg f1: 0.139
		layers = [64, 32]
			activation = sigmoid
				batch size = 128
					Adam=> acc: 0.967, avg recall: 0.967, avg precision: 0.967, avg f1: 0.967
					SGD => acc: 0.105, avg recall: 0.098, avg precision: 0.108, avg f1: 0.031
				batch size = 512
					Adam=> acc: 0.945, avg recall: 0.944, avg precision: 0.944, avg f1: 0.944
					SGD => acc: 0.089, avg recall: 0.099, avg precision: 0.100, avg f1: 0.016
				batch size = 2048
					Adam=> acc: 0.901, avg recall: 0.900, avg precision: 0.899, avg f1: 0.899
					SGD => acc: 0.103, avg recall: 0.100, avg precision: 0.100, avg f1: 0.019
			activation = relu
				batch size = 128
					Adam=> acc: 0.972, avg recall: 0.972, avg precision: 0.972, avg f1: 0.972
					SGD => acc: 0.564, avg recall: 0.690, avg precision: 0.558, avg f1: 0.528
				batch size = 512
					Adam=> acc: 0.966, avg recall: 0.965, avg precision: 0.965, avg f1: 0.965
					SGD => acc: 0.245, avg recall: 0.400, avg precision: 0.246, avg f1: 0.227
				batch size = 2048
					Adam=> acc: 0.948, avg recall: 0.948, avg precision: 0.948, avg f1: 0.948
					SGD => acc: 0.143, avg recall: 0.167, avg precision: 0.149, avg f1: 0.098
		layers = [32, 32, 32]
			activation = sigmoid
				batch size = 128
					Adam=> acc: 0.954, avg recall: 0.954, avg precision: 0.954, avg f1: 0.954
					SGD => acc: 0.102, avg recall: 0.099, avg precision: 0.101, avg f1: 0.020
				batch size = 512
					Adam=> acc: 0.922, avg recall: 0.921, avg precision: 0.921, avg f1: 0.921
					SGD => acc: 0.103, avg recall: 0.100, avg precision: 0.100, avg f1: 0.019
				batch size = 2048
					Adam=> acc: 0.771, avg recall: 0.790, avg precision: 0.765, avg f1: 0.754
					SGD => acc: 0.063, avg recall: 0.083, avg precision: 0.066, avg f1: 0.021
			activation = relu
				batch size = 128
					Adam=> acc: 0.962, avg recall: 0.962, avg precision: 0.961, avg f1: 0.961
					SGD => acc: 0.311, avg recall: 0.461, avg precision: 0.311, avg f1: 0.261
				batch size = 512
					Adam=> acc: 0.951, avg recall: 0.951, avg precision: 0.951, avg f1: 0.951
					SGD => acc: 0.140, avg recall: 0.194, avg precision: 0.147, avg f1: 0.110
				batch size = 2048
					Adam=> acc: 0.941, avg recall: 0.941, avg precision: 0.941, avg f1: 0.940
					SGD => acc: 0.107, avg recall: 0.108, avg precision: 0.104, avg f1: 0.043
epochs = 50
	learning rate = 0.01
		layers = [255]
			activation = sigmoid
				batch size = 128
					Adam=> acc: 0.976, avg recall: 0.976, avg precision: 0.975, avg f1: 0.975
					SGD => acc: 0.917, avg recall: 0.916, avg precision: 0.916, avg f1: 0.916
				batch size = 512
					Adam=> acc: 0.981, avg recall: 0.981, avg precision: 0.981, avg f1: 0.981
					SGD => acc: 0.887, avg recall: 0.886, avg precision: 0.886, avg f1: 0.886
				batch size = 2048
					Adam=> acc: 0.980, avg recall: 0.980, avg precision: 0.980, avg f1: 0.980
					SGD => acc: 0.809, avg recall: 0.819, avg precision: 0.804, avg f1: 0.802
			activation = relu
				batch size = 128
					Adam=> acc: 0.098, avg recall: 0.100, avg precision: 0.100, avg f1: 0.018
					SGD => acc: 0.958, avg recall: 0.958, avg precision: 0.958, avg f1: 0.958
				batch size = 512
					Adam=> acc: 0.980, avg recall: 0.980, avg precision: 0.980, avg f1: 0.980
					SGD => acc: 0.929, avg recall: 0.929, avg precision: 0.929, avg f1: 0.929
				batch size = 2048
					Adam=> acc: 0.981, avg recall: 0.981, avg precision: 0.981, avg f1: 0.981
					SGD => acc: 0.896, avg recall: 0.895, avg precision: 0.894, avg f1: 0.894
		layers = [64, 32]
			activation = sigmoid
				batch size = 128
					Adam=> acc: 0.970, avg recall: 0.970, avg precision: 0.970, avg f1: 0.970
					SGD => acc: 0.900, avg recall: 0.898, avg precision: 0.898, avg f1: 0.898
				batch size = 512
					Adam=> acc: 0.972, avg recall: 0.972, avg precision: 0.972, avg f1: 0.972
					SGD => acc: 0.650, avg recall: 0.699, avg precision: 0.640, avg f1: 0.605
				batch size = 2048
					Adam=> acc: 0.970, avg recall: 0.970, avg precision: 0.970, avg f1: 0.970
					SGD => acc: 0.113, avg recall: 0.101, avg precision: 0.100, avg f1: 0.020
			activation = relu
				batch size = 128
					Adam=> acc: 0.098, avg recall: 0.100, avg precision: 0.100, avg f1: 0.018
					SGD => acc: 0.961, avg recall: 0.960, avg precision: 0.960, avg f1: 0.960
				batch size = 512
					Adam=> acc: 0.976, avg recall: 0.976, avg precision: 0.976, avg f1: 0.976
					SGD => acc: 0.933, avg recall: 0.933, avg precision: 0.932, avg f1: 0.932
				batch size = 2048
					Adam=> acc: 0.975, avg recall: 0.975, avg precision: 0.975, avg f1: 0.975
					SGD => acc: 0.893, avg recall: 0.892, avg precision: 0.892, avg f1: 0.892
		layers = [32, 32, 32]
			activation = sigmoid
				batch size = 128
					Adam=> acc: 0.962, avg recall: 0.961, avg precision: 0.961, avg f1: 0.961
					SGD => acc: 0.573, avg recall: 0.574, avg precision: 0.564, avg f1: 0.518
				batch size = 512
					Adam=> acc: 0.959, avg recall: 0.958, avg precision: 0.958, avg f1: 0.958
					SGD => acc: 0.113, avg recall: 0.101, avg precision: 0.100, avg f1: 0.020
				batch size = 2048
					Adam=> acc: 0.958, avg recall: 0.958, avg precision: 0.958, avg f1: 0.958
					SGD => acc: 0.113, avg recall: 0.101, avg precision: 0.100, avg f1: 0.020
			activation = relu
				batch size = 128
					Adam=> acc: 0.961, avg recall: 0.961, avg precision: 0.961, avg f1: 0.961
					SGD => acc: 0.959, avg recall: 0.959, avg precision: 0.959, avg f1: 0.959
				batch size = 512
					Adam=> acc: 0.963, avg recall: 0.963, avg precision: 0.962, avg f1: 0.962
					SGD => acc: 0.937, avg recall: 0.936, avg precision: 0.936, avg f1: 0.936
				batch size = 2048
					Adam=> acc: 0.959, avg recall: 0.960, avg precision: 0.959, avg f1: 0.959
					SGD => acc: 0.886, avg recall: 0.885, avg precision: 0.885, avg f1: 0.884
	learning rate = 0.001
		layers = [255]
			activation = sigmoid
				batch size = 128
					Adam=> acc: 0.981, avg recall: 0.981, avg precision: 0.981, avg f1: 0.981
					SGD => acc: 0.847, avg recall: 0.848, avg precision: 0.843, avg f1: 0.843
				batch size = 512
					Adam=> acc: 0.979, avg recall: 0.979, avg precision: 0.979, avg f1: 0.979
					SGD => acc: 0.688, avg recall: 0.712, avg precision: 0.680, avg f1: 0.661
				batch size = 2048
					Adam=> acc: 0.970, avg recall: 0.969, avg precision: 0.970, avg f1: 0.970
					SGD => acc: 0.314, avg recall: 0.372, avg precision: 0.307, avg f1: 0.249
			activation = relu
				batch size = 128
					Adam=> acc: 0.982, avg recall: 0.982, avg precision: 0.981, avg f1: 0.981
					SGD => acc: 0.909, avg recall: 0.908, avg precision: 0.907, avg f1: 0.907
				batch size = 512
					Adam=> acc: 0.980, avg recall: 0.980, avg precision: 0.980, avg f1: 0.980
					SGD => acc: 0.861, avg recall: 0.860, avg precision: 0.859, avg f1: 0.859
				batch size = 2048
					Adam=> acc: 0.978, avg recall: 0.978, avg precision: 0.978, avg f1: 0.978
					SGD => acc: 0.670, avg recall: 0.715, avg precision: 0.665, avg f1: 0.662
		layers = [64, 32]
			activation = sigmoid
				batch size = 128
					Adam=> acc: 0.973, avg recall: 0.973, avg precision: 0.973, avg f1: 0.973
					SGD => acc: 0.352, avg recall: 0.439, avg precision: 0.340, avg f1: 0.275
				batch size = 512
					Adam=> acc: 0.972, avg recall: 0.972, avg precision: 0.972, avg f1: 0.972
					SGD => acc: 0.097, avg recall: 0.092, avg precision: 0.100, avg f1: 0.018
				batch size = 2048
					Adam=> acc: 0.961, avg recall: 0.960, avg precision: 0.960, avg f1: 0.960
					SGD => acc: 0.113, avg recall: 0.101, avg precision: 0.100, avg f1: 0.020
			activation = relu
				batch size = 128
					Adam=> acc: 0.971, avg recall: 0.971, avg precision: 0.971, avg f1: 0.971
					SGD => acc: 0.907, avg recall: 0.905, avg precision: 0.905, avg f1: 0.905
				batch size = 512
					Adam=> acc: 0.971, avg recall: 0.971, avg precision: 0.971, avg f1: 0.971
					SGD => acc: 0.792, avg recall: 0.792, avg precision: 0.785, avg f1: 0.778
				batch size = 2048
					Adam=> acc: 0.965, avg recall: 0.965, avg precision: 0.965, avg f1: 0.965
					SGD => acc: 0.322, avg recall: 0.371, avg precision: 0.312, avg f1: 0.268
		layers = [32, 32, 32]
			activation = sigmoid
				batch size = 128
					Adam=> acc: 0.962, avg recall: 0.962, avg precision: 0.962, avg f1: 0.962
					SGD => acc: 0.113, avg recall: 0.101, avg precision: 0.100, avg f1: 0.020
				batch size = 512
					Adam=> acc: 0.962, avg recall: 0.962, avg precision: 0.962, avg f1: 0.962
					SGD => acc: 0.097, avg recall: 0.100, avg precision: 0.100, avg f1: 0.018
				batch size = 2048
					Adam=> acc: 0.952, avg recall: 0.952, avg precision: 0.952, avg f1: 0.952
					SGD => acc: 0.103, avg recall: 0.100, avg precision: 0.100, avg f1: 0.019
			activation = relu
				batch size = 128
					Adam=> acc: 0.965, avg recall: 0.965, avg precision: 0.964, avg f1: 0.964
					SGD => acc: 0.891, avg recall: 0.890, avg precision: 0.890, avg f1: 0.890
				batch size = 512
					Adam=> acc: 0.965, avg recall: 0.965, avg precision: 0.965, avg f1: 0.965
					SGD => acc: 0.743, avg recall: 0.747, avg precision: 0.740, avg f1: 0.737
				batch size = 2048
					Adam=> acc: 0.962, avg recall: 0.962, avg precision: 0.962, avg f1: 0.961
					SGD => acc: 0.232, avg recall: 0.270, avg precision: 0.228, avg f1: 0.177
